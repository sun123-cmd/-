**强化学习作业 2024夏**

| 题目     | Online Reinforcement Learning for Mixed Policy Scopes |
| -------- | :---------------------------------------------------- |
| **姓名** | **孙文昊**                                            |
| **学号** | **2023E8013282127**                                   |

[TOC]

## 1. 摘要

组合疗法指的是使用多种治疗手段——比如手术、药物治疗和行为疗法——来治疗单一疾病，已经成为治疗包括癌症、HIV和抑郁症在内的各种疾病的基础。所有可能的治疗组合导致了一系列具有混合范围的治疗策略（即政策），或者医生可以根据上下文观察到的行动以及他们应该采取的行动。在本文中，我们研究了具有混合范围的在线强化学习设置，用于优化政策空间。特别是，我们开发了新的在线算法，与部署在环境中的最优代理相比，实现了亚线性的遗憾界限。遗憾界限依赖于与混合范围相关的诱导状态-动作空间的最大基数。我们进一步引入了一种规范表示，用于给定因果图的任意干预分布的子集，这导致了模型参数的非平凡、最小表示。引言

政策学习问题涉及基于环境状态选择行动，目标是优化某个性能度量。现有的政策学习方法通常可以分为在线学习和离线学习。在线强化学习（RL）通过进行顺序实验学习最优策略，同时根据观察到的结果不断调整当前部署的策略。已经为各种规范环境开发了有效的在线算法，包括多臂老虎机[40, 25, 1, 26]、马尔可夫决策过程（MDPs）[45, 22, 49]和部分可观察MDP[17, 3]。另一方面，离线学习专注于离线设置，目标是从不同行为策略收集的历史数据中评估候选政策的有效性[38, 47, 48]。更一般地说，因果推断（CI）学科提供了一套引人注目的工具和正式语言，用于离线学习。它允许代理从观察和对数据生成机制的了解中得出新政策的结论。已经提出了几种方法和图形标准[36, 44, 8]。

总的来说，几乎所有上述方法都关注于优化具有固定状态-动作空间的参数化策略空间，这被称为政策范围。也就是说，行动和观察到的状态集合是预先指定的。然而，在许多实际应用中，这可能有些过于严格，代理必须优化具有不同状态-动作空间的候选政策；这些被称为混合范围。为了具体起见，考虑图1a中描述酒精使用障碍可能治疗的因果模型[32]。根据酒精依赖患者的状况Z，医生可能会开处方药物X1以最大化总的戒断天数Y。另一种治疗是开处方行为疗法X2，这改变了患者的社会环境W，进而改变了饮酒模式。分析的目的是评估同时开处方药物和行为疗法（即策略⇡ = {⇡1(X1 | Z), ⇡2(X2 | Z)}）的组合治疗效果，与分别开处方每种治疗的效果（即策略⇡ = {⇡1(X1 | Z)}或⇡ = {⇡2(X2 | Z)}）进行比较。在这个例子中，所有三种治疗方案的行动范围是不同的，分别对应于{X1, X2}、{X1}和{X2}。医生必须确定干预的最佳行动集合，以找到最优策略，以最大化戒断天数。

越来越多的文献关注在各种关于底层环境的假设下优化混合策略的问题。例如，在具有线性奖励函数的多臂赌博机情况下，已经存在有效的在线算法可以确定一个最优的动作子集，从而通过干预这些动作来最大化期望奖励 [11, 10]。最近，在基于因果模型的结构约束下，策略优化有了新的进展。非参数的因果关系知识已经被利用来在日益宽松的情景下精炼具有混合范围的策略空间 [28, 29, 30]。

特别是，现在存在一些图形条件，借助这些条件可以：

**（1）**识别一组动作的必要状态

**（2）**确定相对于其最大可实现期望奖励（最优性）的策略范围的部分排序。

这些结果使得代理能够检测和移除次优和低效的范围，并将其注意力集中在可能的最优动作上，从而加速学习过程。

尽管已经取得了实质性的进展，但在任意因果系统中学习具有混合范围的最优策略的有效在线算法仍然缺失。在图形特征方面，候选策略空间的精炼如何影响代理的表现也极其复杂，并且仍未为人所知。本文的目标是解决这些挑战。我们将研究在提供一个与底层结构因果模型（SCM）相关的因果图的情况下，具有混合范围的最优策略的在线学习。具体来说，我们的贡献总结如下：

**（1）**我们开发了一种新的在线学习算法，能够在未知的SCM中识别具有混合范围的最优策略，并且证明它达到了次线性累积后悔值。

**（2）**我们引入了一种新的具有有限潜在状态的SCM参数化方法，该方法可以通过最小的c-组件集合表示任意的干预分布子集。

**（3）**利用这些结果，我们开发了一种替代的在线算法，该算法在计算上更高效，同时达到了相同的累积后悔值渐近界限。由于篇幅限制，所有证明都在附录A中提供。

### 1.1.准备工作

在本节中，我们介绍贯穿全文使用的基本符号和定义。我们使用大写字母表示随机变量（如X），小写字母表示其取值（如x），⌦X表示X的定义域。对于任意集合X，记其基数为|X|。我们用P(X)表示变量X上的概率分布。同样地，P(Y | X)表示一组条件分布P(Y | X = x)对所有取值x的集合。我们将一致地使用P(x)作为概率P(X = x)的缩写；同样，P(Y = y | X = x) = P(y | x)。最后，1{Z = z}是一个指示函数，当事件Z = z成立时返回1，否则返回0。

我们的基本语义框架基于结构因果模型（SCMs）[36, 8]。一个SCM M是一个四元组hV, U, F, P(U)i，其中V是内生变量的集合，U是外生变量的集合。F是函数的集合，使得每个fV ∈ F决定一个内生变量V ∈ V的值，该值以系统中其他变量的组合作为参数。也就是说，V = fV(PAV, UV)，其中PAV ⊆ V，UV ⊆ U。外生变量U ∈ U彼此独立，其值从外生分布P(U)中抽取。自然地，M在内生变量V上诱导出联合分布P(V)，称为观测分布。

在一个子集X ⊆ V上的策略π是决策规则的序列{π(X | SX)}X∈X，其中每个π(X | SX)是一个概率分布，从协变量集合SX ⊆ V的定义域映射到动作X的定义域。遵循策略π对变量X进行干预，记作do(π)，是一个操作，将每个X ∈ X的值设置为由策略X ∼ π(X | SX)决定的值，替换通常确定其值的函数fX = {fX : ∀X ∈ X}。对于一个SCM M，记Mπ为由干预do(π)引起的M的子模型。对于一个集合Y ⊆ V，干预分布P(Y |do(π))定义为子模型Mπ中Y的分布，即PM(Y |do(π))，PMπ(Y)。当上下文明显时，省略下标M。

每个SCM M也与一个因果图G（例如图1a）相关联，这是一个有向无环图（DAG），其中实心节点表示内生变量V，空心节点表示外生变量U，箭头表示每个结构函数fV的参数PAV, UV。我们将使用标准的图论家庭缩写表示图形关系，如父母节点、子节点、后代和祖先。例如，图中X的父节点集合记作pa(X)G = ∪X∈X pa(X)G；ch, de和an也类似定义。大写版本Pa, Ch, De, An包括参数，例如Pa(X)G = pa(X)G ∪ X。对于子集X ⊆ V，G[X]是从G中诱导的子图，包含节点X及其间的边。

为了方便起见，我们定义在内生节点Vi, Vj ∈ V之间的双向箭头Vi ↔ Vj为一系列箭头Vi ← Uk → Vj，其中Uk ∈ U是Vi, Vj共享的外生父节点。双向路径是连续的双向箭头序列。我们将利用图G中的一种特殊类型的节点聚类，称为c-组件[53]。形式上，如果在因果图G中任意一对Vi, Vj ∈ C由双向路径连接，则子集C ⊆ V是一个c-组件。例如，在图1a中的因果图G中，存在双向路径X1 ↔ X2 ↔ Y和Z ↔ W。因此，G包含c-组件{X1, X2, Y}，{Z, W}。另一方面，图1b中的图GS1包含c-组件{X1}，{X2, Y}，{Z, W}，因为双向箭头X1 ↔ X2和Y ↔ X1被移除。关于SCMs的详细综述，请参阅[36, Ch. 7]。

## 2 优化混合策略空间

我们关注的是一个决策制定环境，其中代理与一个结构因果模型（SCM）交互以优化奖励Y。代理可以对任意子集的动作X进行干预，这些子集被称为可干预变量。策略范围S是一组对hX, S_X i的集合，其中X ∈ X且协变量S_X ⊆ V ⊆ {Y}。令X(S) = {X}hX,S_X i ∈ S表示范围S中的动作子集；同样，S(S) = ∪hX,S_X i ∈ S S_X。例如，在图1b中的范围S_1 = {hX_1, {Z}i}中，X(S_1) = {X_1}，S(S_1) = {Z}。形式上，与范围S相关联的策略π是一系列决策规则{π(X | S_X)} hX,S_X i ∈ S。此类策略的集合π定义了一个策略空间Π_S。Π_S中的所有策略共享相同的范围S：它们基于协变量S(S)的观测值对X(S)中的所有动作进行干预。策略空间Π_S模拟了许多经典顺序决策环境中的一般决策规则和治疗方案，包括马尔可夫决策过程中的策略[39]、影响图中的策略配置[24]和动态治疗方案[33]。

本文研究了一个更一般的策略空间，不局限于单一范围S。混合策略范围是一个对可干预变量X的策略范围的组合集合。形式上，

#### 定义1

混合策略范围S是一个策略范围集合S，使得X(S) ⊆ X。

关于混合策略范围S的策略空间Π_S是与S中的每个范围兼容的所有策略的集合，即Π_S = ∪S ∈ S Π_S。因此，我们将始终把Π_S称为混合策略空间[30]。我们的目标是在Π_S中学习一个最优策略，以在SCM M中最大化奖励Y，即

$$
\pi^* = \arg \max_{\pi \in \Pi_S} E_M[Y \mid do(\pi)]
$$
代理无法访问底层SCM M的参数化。相反，代理可以观察到内生变量V、混合策略空间Π_S和代表M中编码的定性知识的因果图G。本文假设内生变量V是离散且有限的，而外生变量U可以取任何（连续的）值。因此，分布P(V)和P(V | do(π))是分类概率度量。

### 2.1 因果上置信区间算法

典型的在线算法通过直接在实际环境（SCM M）中部署策略来评估候选策略π ∈ Π_S的有效性，通过重复多轮干预t = 1, 2, ..., T来实现。在每轮t中，算法选择一个策略π_t ∈ Π_S，执行干预do(π_t)，并接收随后的奖励Y_t。在线算法在SCM M中经过T轮干预后的累积遗憾定义为:
$$
R(T, M) = \sum_{t=1}^{T} \left( E_M[Y \mid do(\pi^*)] - Y_t \right)
$$
。换句话说，R(T,M)衡量了由于在线算法并不总是选择最优策略π*而产生的累积损失。一个合理或理想的在线算法的属性是对于任意SCM M具有次线性遗憾，即:
$$
\lim_{T \to \infty} \frac{R(T, M)}{T} = 0
$$
如果遗憾是次线性的，意味着随着T趋向于无穷大，代理几乎总是选择最优策略。

我们将介绍一种新的在线算法，该算法在优化混合策略空间的同时实现次线性遗憾。它遵循面对不确定性的乐观原则。也就是说，对于每个回合，代理基于上置信区间（UCB）选择策略，这些区间在与过去观测兼容的最乐观模型中评估候选策略的有效性，从而产生最大的期望奖励。我们的方法的一个创新之处是利用嵌入在因果结构G中的不变性，这使我们能够使用由不同范围的其他策略诱导的样本来评估候选策略。

形式上，令C(G)表示包含因果图G中所有（最大）c-组件的c-集合。结果，C(G)在G的节点上形成一个分区{C1, ..., Cn}，其中每对Ci和Cj（当i ≠ j时）之间没有双向箭头。再次考虑图1a中的因果图G，c-集合C(G)包含c-组件`C1 = {X1, X2, Y}，C2 = {Z, W}`，这在G的节点上形成一个分区。对于任意子集C ⊆ V，c-因子Q[C]定义为
$$
Q[C](v) = P(c \mid do(v 、setminus c))
$$
，其中
$$
do(v \setminus c)
$$
是一个原子干预。将`V \ C`的值设置为常量`v \ c`。为了方便起见，我们通常省略输入v并写作Q[C]。固定策略范围`S ∈ S`。对于任何策略`π ∈ Π_S`，令集合`Z = An(Y)_(G_S) \ X(S)`。干预分布`P(Y | do(π))`可以在子图G[Z]中的c-组件上分解如下：
$$
P(y \mid do(\pi)) = \sum_z \prod_{X \in X(S)} \pi(x \mid s_X) \prod_{C \in C_S} Q[C]
$$
其中c-集合`C_S = C(G[Z])`包含G[Z]中的所有c-组件。令c-集合`C = ∪_S∈S C_S`。在上述方程中的量中，决策规则π(x | s_X)是固定的。为了评估Π_S中候选策略的效果`P(Y | do(π))`，因此只需学习c-组件C ∈ C的c-因子Q[C]的参数。再次考虑图1a中的因果图G；混合策略范围S包含元素`S1 = {hX1, {Z}i}`，`S2 = {hX2, {Z}i}`和`S1,2 = {hX1, {Z}i, hX2, {X1, Z}i}`。对于具有范围`S1 = {hX1, {Z}i`}的策略`π1(x1|z)，P(Y | do(π1)`)分解如下：

$$
P(y \mid do(\pi_1)) = \sum_{x_1, x_2, z, w} \pi(x_1 \mid z) Q[X_2, Y] Q[Z, W]
$$
c-集合`C_S1 = {{X2, Y}, {Z, W}}`。类似地，枚举与每个范围S ∈ S相关的c-组件得到一个c-集合`C = {{Z, W}, {X1, Y}, {X2, Y}, {Y}}`。对于任何C ∈ C，令S(C)为一个范围子集S ∈ S，使得C ∈ C_S。我们的在线算法通过直接在实际SCM M中执行干预do(π)来评估Q[C]。

****

#### 算法1 CAUSAL-UCB*

输入：因果图G，策略空间Π_S，失败容忍度ε ∈ (0, 1)。

1. 对于所有回合t = 1, 2, ...，执行以下步骤：
2. 对每个C ∈ C，按照方程(8)计算经验估计Q̂_t[C]。
3. 令M_t为所有与G相关的SCM集合，使得每个c-因子Q_M[C]与其估计Q̂t[C]接近。即，对于每个V ∈ C，对于任何pa^+_V:

$$
\|q(⋅ \mid pa^+_V) - \hat{q}_t(⋅ \mid pa^+_V)\|_1 \leq \sqrt{\frac{6 |Ω_V| \ln \left(2 |Ω_{PA^+_V}| |V(C)| \frac{t}{\epsilon} \right)}{\max(n_t(pa^+_V), 1)}}
$$


4. 找到乐观策略π_t，使得

$$
\pi_t = \arg\max_{\pi \in \Pi_S} \max_{M \in M_t} E_M[Y \mid do(\pi)]。
$$

5. 执行do(π_t)并观察V_t。

****

例如，再次考虑图1中描述的因果图G和策略范围`S1, S2, S1,2`。对于c-组件`C3 = {X2, Y} ∈ C_S1`，这意味着策略范围S1 ∈ S(C3)。观察到CY = {X2, Y}，CX2 = {X2}，并且扩展父节点`PA^+_Y = {X2, Z, W}，PA^+_X2 = {Z}`。引理1表示可以从任何策略π1 ∈ Π_S1的采样过程中计算Q[C3]，即

$$
Q[X_2, Y] = P(y \mid x_2, z, w, do(\pi_1))P(x_2 \mid z, do(\pi_1))。
$$
我们提出的算法CAUSAL-UCB*的详细信息在算法1中提供。它通过在重复回合t = 1, ..., T中与底层SCM进行交互。在回合t中，它计算每个c-组件C ∈ C的c-因子的经验估计Q̂_t[C]。更具体地说，回合t之前的c-因子Q̂_t[C]的经验估计为：

$$
\hat{Q}_t[C](v) = \prod_{V \in C} \hat{q}_t(v \mid pa^+_V)，其中 \hat{q}_t(v \mid pa^+_V) = \frac{n_t(v, pa^+_V)}{\max(n_t(pa^+_V), 1)}。
$$
在上述方程中的量中，
$$
n_t(v, pa^+_V)
$$
是在回合t之前部署策略π ∈ Π_S(C)时观测到_
$$
V = v，PA^+_V = pa^+_V
$$
的事件计数。即
$$
n_t(v, pa^+_V) = \sum_{{\tau}=1}^{t-1} 1_{V_{\tau} = v, PA^+_{V_{\tau}} = pa^+_V, \pi_{\tau} \in \Pi_S(C)} \\
n_t(pa^+_V) = \sum_v n_t(v, pa^+_V)
$$
在步骤3中，CAUSAL-UCB使用以每个估计Q̂_t[C]为中心的凸区间维护所有与G兼容的可能SCM的置信集M_t。然后它在M_t中找到诱导最大期望奖励E[Y | do(π)]的最优策略π_t（步骤4）。最后，步骤7在整个回合t中执行do(π_t)并收集新样本V_t。令V(C)为并集∪_C∈C C。在T > 1个回合之后，可以推导出CAUSAL-UCB的遗憾界限。

#### 定理1

对于因果图G和混合策略范围S，固定ε ∈ (0, 1)。以至少1 - ε的概率，对于任何T > 1，CAUSAL-UCB*的遗憾被以下界限所约束：
$$
R(T, M) \leq 19\phi(G, S) \sqrt{|S| T \ln\left(\frac{|V(C)| T}{\epsilon}\right)}
$$
其中函数φ(G, S) 为：
$$
\phi(G, S) = \max_{{S \in S}} \phi(G, S)
$$
且：
$$
\phi(G, S) = \sum_{{V \in V(C_S)}} \sqrt{|Ω_V| \cdot |PA^+_V|}
$$
**定理1表明**：CAUSAL-UCB*能够实现次线性遗憾；因此，策略π_t最终收敛于最优策略，当回合数t趋向于无穷大时。例如，在图1中描述的因果图G和混合策略范围S = {S1, S2, S1,2}中，应用定理1得出的遗憾界限为：
$$
O(\phi(G, S) \sqrt{T \ln T})
$$
评估φ(G, S)时，基数|Ω_X2| > |Ω_X1|给出：
$$
\phi(G, S) = \phi(G, S_1) = \sqrt{|Ω_{Y, X_2, Z, W}|} + \sqrt{|Ω_{X_2, Z}|} + \sqrt{|Ω_{W, Z, X_1}|} + \sqrt{|Ω_{Z}|}
$$
相比之下，应用标准UCB算法，用确定性策略在Π_S中作为手臂，遗憾界限为：
$$
O(\sqrt{|\Pi_S| T \ln T})
$$
其中：
$$
|\Pi_S| = |Ω_{X1}| |Ω_{Z}| + |Ω_{X2}| |Ω_{Z}| + |Ω_{X1}| |Ω_{Z}| |Ω_{X2}| |Ω_{Z}|
$$
，其值随着内生状态数目的增加被CAUSAL-UCB*的遗憾界限所支配。

#### 加速学习过程

在定理1的遗憾界限中，函数φ(G, S)可以被视为评估学习任务难度的度量。它依赖于与策略范围S相关的状态-动作空间的最大基数。存在关于混合范围的一般特征，它允许代理检测冗余和次优的策略范围[30]。通过这样做，我们可以获得代理候选策略范围的精炼，使其更快且更稳健地收敛到最优策略。例如，考虑图2中描述的因果图G。我们感兴趣的是评估具有混合范围:
$$
S = \{S_1, S_2, S_{1,2}\}, S_1 = \{hX_1, ∅i\}，S_2 = \{hX_2, ∅i\}, S_{1,2} = \{hX_1, ∅i, hX_2, \{X_1\}i\}
$$
。可以证明S1,2是冗余的，因为总是存在一个具有范围S1或S2的最优策略。移除S1,2会导致更精炼的混合范围S* = {S1, S2}。由于:
$$
\phi(G, S^*) = \max\{\phi(G, S_1), \phi(G, S_2)\} \leq \max\{\phi(G, S_1), \phi(G, S_2), \phi(G, S_{1,2})\} = \phi(G, S)
$$
应用CAUSAL-UCB*与精炼范围S*相比使用S会实现更小的遗憾，从而加速在线学习过程。

通过这种方式，代理能够更加有效地检测和移除冗余的策略范围，专注于可能的最优动作，从而显著提高学习效率和收敛速度。

## 3 乐观规划混合策略空间

由算法CAUSAL-UCB*所涉及的优化问题（方程(5)）要求学习者在所有与因果图和实验数据兼容的SCM中进行搜索。从原则上讲，这是一项具有挑战性的任务，因为我们无法访问底层结构函数F或外生分布P(U)的参数形式。因此，不清楚现有的优化过程如何能够被使用。

当内生变量的定义域是离散且有限时，存在一个典型的SCM家族，它使用有限数量的外生状态对任何因果图中的所有观测和干预分布进行参数化[60, 41]。一个c-组件C ∈ C被认为覆盖一个外生变量U ∈ U，如果U ∈ ∪_{V ∈ C} U_V。令C(U)表示C中覆盖U的c-组件的集合。再次考虑图1a中的因果图。对于c-集合C = {{Z, W}, {X1, Y}, {X2, Y}, {Y}}，外生节点U2由C(U2) = {{X1, Y}, {X2, Y}, {Y}}覆盖；同样地，C(U1) = {{Z, W}}。接下来，我们提出一个新的有限状态分解，表示任意c-集合C中的c-因子。

#### 定理2

对于任意SCM M = hV, U, F, P(U)i，令C为任意c-集合。对于任意C ∈ C，c-因子Q[C]分解如下：

$$
Q[C](v) = \sum_{U \in U} \sum_{u=1}^{d_U} \prod_{V \in C} 1_{f_V(pa_V, u_V) = v} \prod_{U \in U} P(u)
$$
其中对于每个外生U ∈ U，P(U)是一个离散分布，定义域为{1, ..., d_U}，基数d_U = ∑_{C ∈ C(U)} |Ω_{Pa(C)}|；C(U) ⊆ C是覆盖U的c-组件。

因此，我们将定理2中定义的具有有限外生定义域的SCM家族称为C-典型SCM。为了具体说明，考虑图1a中的因果图，其中X1, X2, Y, Z, W ∈ {0, 1}。回想一下，对于C = {{Z, W}, {X1, Y}, {X2, Y}, {Y}}，C(U1) = {{Z, W}}和C(U2) = {{X1, Y}, {X2, Y}, {Y}}。在C-典型SCM中，U1的基数为d1 = |Ω_{Pa(Z, W)}| = |Ω_{Z, X1, W}| = 8。同样地，在C-典型SCM中，U2的基数为：

$$
d_2 = |Ω_{Pa(X_1, Y)}| + |Ω_{Pa(X_2, Y)}| + |Ω_{Pa(Y)}| = 2^5 + 2^4 + 2^4 = 64。
$$
定理2表明，在图1a的图中，Q[Z, W], Q[X2, Y]可以表示为：

$$
Q[Z, W] = \sum_{u1=1}^{d1} 1_{f_Z(u1) = z} 1_{f_W(z, x1, u1) = w} P(u1)
$$

$$
Q[X2, Y] = \sum_{u2=1}^{d2} 1_{f_{X2}(z, u2) = x2} 1_{f_{Y}(x2, z, w, u2) = y} P(u2)
$$

其中P(U1), P(U2)是在有限定义域{1, ..., d1}, {1, ..., d2}上的分布。根据方程(3)的分解，对于任何策略π1 ∈ Π_S1，干预分布P(Y | do(π1))是c-因子Q[X2, Y], Q[Z, W]的函数，给出如下：

$$
P(y \mid do(\pi_1)) = \sum_{x1, x2, z, w} \pi(x1 \mid z) \sum_{u1=1}^{d1} 1_{f_Z(u1) = z} 1_{f_W(z, x1, u1)} P(u1) \sum_{u2=1}^{d2} 1_{f_{X2}(z, u2) = x2} 1_{f_{Y}(x2, z, w, u2) = y} P(u2)
$$
观察定理2中的内容，外生定义域的基数依赖于C中的c-组件总数。接下来，我们介绍一种通过探索C中包含的c-因子Q[C]之间的可识别关系来减少典型SCM模型复杂性的方法。

#### 定义2

对于因果图G和c-集合C，对于所有C ∈ C，c-因子Q[C]是可识别的，如果Q[C]能够从C \ {C}中的其他c-因子Q[C']唯一计算得到。即，对于每对SCM{M1, M2}，它们的G_M1 = G_M2 = G，并且对于任何
$$
C' \in C \setminus \{C\}, \quad Q_{M1}[C'] = Q_{M2}[C']
$$
，那么
$$
Q_{M1}[C] = Q_{M2}[C]
$$
换句话说，如果Q[C]可以写成集合C0 = C \ {C}中c-因子的函数，则Q[C]相对于hG, Ci是可识别的。例如，考虑图1a的因果图和c-集合C = {{Z, W}, {X1, Y}, {X2, Y}, {Y}}。研究工作表示c-因子Q[Y]可以从Q[X1, Y]中识别，并且表示为：
$$
Q[Y] = \sum_{x1} Q[X_1, Y]
$$
对于每对生成：
$$
C_0 = \{\{Z, W\}, \{X_1, Y\}, \{X_2, Y\}\}
$$
中c-因子集合的SCM，它们在Q[Y]的参数上也必须一致。因此，仅表示子集C0中的c-因子的参数就足够了，我们称之为原始c-集合C的简化。

#### 定义 3

对于一个因果图 \(G\) 和一个 \(c\)-集合 \(C\)，如果通过连续移除可识别的 \(c\)-分量 \(C\) 得到一个子集 \(C'\subseteq C\)，则称其为 \(C\) 的一个减少。

一个减少 \(C'\) 是最小的，如果不存在真子集
$$
C'' \subset C'
$$
使得 \(C''\) 是 \(C'\) 的一个减少。为了降低候选结构因果模型 (SCM) 的模型复杂性，通常需要以最小减少的形式表示 \(c\)-分量。接下来我们描述一种系统化的程序，以在因果图 \(G\) 中获得 \(c\)-集合 \(C\) 的最小减少。我们算法 MINCOLLECT 的详细信息见算法2。它通过反复移除集合 \(C\) 中可识别的 \(c\)-因子Q[C]来实现这一目标。子程序 IDENTIFY是一个确定 Q[C]是否可以从另一个 \(c\)-因子(Q[C'] 中识别出来的完整算法，其中：
$$
\C \subseteq C'\
$$
特别地，如果目标查询是可识别的，IDENTIFY 返回一个从Q[C'] 估计Q[C] 的公式；否则，它返回“FAIL”。可以证明，算法2中的贪心过程总是返回 \(C\) 的一个最小减少。

#### 推论 1

对于一个因果图 \(G\) 和一个 \(c\)-集合 \(C\)，MINCOLLECT\((G,C)\) 返回 \(C\) 的一个最小减少 \(C^*\)。

#### 推论 2

对于一个因果图 \(G\)，任意 \(c\)-集合 \(C\) 都有唯一的最小减少。再次考虑图1a中的因果图 \(G\) 中的 \(c\)-集合:

$$
\C = \{\{Z, W\}, \{X_1, Y\}, \{X_2, Y\}, \{Y\}\}
$$
应用过程 MINCOLLECT(C, G) 得到一个最小减少 
$$
C^* = \{\{Z, W\}, \{X_1, Y\}, \{X_2, Y\}\}
$$
。因此，在计算方程(5)中的乐观策略时，只需考虑与 \(G\) 兼容的 \(C^*\)-规范 SCMs。相比于 \(C\)-规范 SCMs，在 \(C^*\)-规范 SCMs 中，\(U_1\) 的基数保持不变，等于 
$$
d_1 = |\Omega_{Pa(Z,W)}| = 8
$$
另一方面,
$$
C^*(U_2) = \{\{X_1, Y\}, \{X_2, Y\}\}
$$
 且 \(U_2\) 的基数为:
$$
d_2 = |\Omega_{Pa(X_1,Y)}| + |\Omega_{Pa(X_2,Y)}| = 2^5 + 2^4 = 48
$$
，这比方程(12)中 \(C\)-规范 SCMs 中 \(U_2\) 的基数要小。然后可以通过求解一系列等效的多项式程序来获得方程(5)中的乐观策略。关于规范 SCMs 及相关工作的更详细调查，请参阅附录C。

### 3.1 汤普森抽样

因子表示法允许方程(5)中的优化问题简化为一系列等效的多项式程序。然而，解决多项式优化问题一般是NP困难的，这意味着应用CAUSAL-UCB*在计算上仍然具有挑战性。本节介绍了一种计算可行的在线算法，同时在累积后悔上达到类似的渐近界限。

我们的算法基于Thompson采样（TS）的启发式方法。它在所有与因果图 \(G\) 兼容的可能的SCMs \(M\) 上维持一个先验分布。有人可能会认为定义这样的先验是有挑战性的，因为代理无法访问底层SCM的参数形式。幸运的是，根据定理2的分解，我们可以假设外生域是离散且有限的，而不失一般性。特别是，我们将考虑一组 \(C^*\)-规范SCMs，其中 \(C^*\) 是因果图 \(G\) 中 \(c\)-集合 \(C\) 的最小减少。每个*U*的域的基数由 
$$
d_U = \sum_{C \in C^*(U)} |\Omega_{Pa(C)}|
$$
给出。我们假设 \(P(U)\) 的概率从非信息性Dirichlet先验中抽取； \(F\) 从可能的结构函数的有限类中均匀抽取。也就是说，对于每个
$$
U \in \mathcal{U} ,  V \in \mathcal{V}
$$

$$
P(U) \sim \text{Dir}(\alpha_1, \ldots, \alpha_{d_U}), \quad f_V \sim \text{Unif}(\Omega_{Pa(V)} \times \Omega_{U_V} \to \Omega_V)
$$

其中:
$$
alpha_1 = \ldots = \alpha_{d_U} = 1；\Omega_{Pa(V)} \times \Omega_{U_V} \to \Omega_V
$$
是包含从 \(Pa(V), U_V\) 的有限域到 \(V\) 的所有函数的类，这个类必须只包含有限数量的元素。

---

算法3 CAUSAL-TS*
1. 输入：因果图 \(G\)，策略空间 \(\Pi_S\)，先验 \(\rho\)。

2. 对于所有的阶段 \(t = 1, 2, \ldots\)：

3. 从更新后的后验分布中抽取一个
   $$
   SCM \(M_t \sim \rho(M | \bar{V}_t)
   $$

4. 计算一个最优策略 \(\pi_t\) 使得
$$
  \pi_t = \arg \max_{\pi \in \Pi_S} \mathbb{E}_{M_t}[Y | do(\pi)]
$$

5. 执行 $\(do(\pi_t)\) 并观察 \(V^{(t)}\)。

---

算法3（CAUSAL-TS*）的细节如上所述。在每个阶段 \(t\)，它在阶段 \(t\) 之前从收集的样本：
$$
\bar{V}_t = \{V_1, \ldots, V_{t-1}\}
$$
更新后验分布：
$$
\rho(M | \bar{V}_t)
$$
，并从更新后的后验分布中抽取一个估计 \(M_t\)。类似于贝叶斯因果推断方法，我们将使用Gibbs采样获得一个后验样本：
$$
M \sim \rho(M | \bar{V}_t)
$$
在步骤4中，CAUSAL-TS* 计算一个乐观策略 \(\pi_t\)，该策略在采样的SCM \(M_t\) 上最大化期望结果 ：
$$
\mathbb{E}[Y | do(\pi)]
$$
最后，代理在整个阶段 \(t\) 执行 \(\pi_t\) 并收集新的样本 \(V_t\)。根据之前的工作，我们将使用贝叶斯累积后悔来评估TS的性能，即：
$$
R(T, \rho) = \mathbb{E}[R(T, M) | M \sim \rho(M)]
$$
，其中期望是相对于 \(M\) 上的先验分布来取的。TS和UCB算法在许多模型类中存在一般关系。这允许将为CAUSAL-UCB*开发的后悔界转换为CAUSAL-TS*的贝叶斯后悔界。正式地，

#### 定理3

 给定因果图 \(G\)，混合策略范围 \(S\)，和先验分布 \(\rho\)，对于任意 \(T > 1\)，CAUSAL-TS*的后悔被界定为
$$
R(T, \rho) \leq 26 \beta(G, S) \sqrt{|S| T \ln \left(\frac{|V(C)|T}{|S|}\right)}.
$$
与定理1相比，上述后悔界意味着CAUSAL-TS*在渐近性能上与CAUSAL-UCB*相似。特别地，算法3要求仅在特定的SCM \(M_t\) 中找到一个最优策略，而在算法1中，M_t的参数是不精确的，被限定在假设类 \(M_t\) 中。存在有效的规划算法可以在提供底层SCM详细参数化的结构化环境中找到最优策略。这意味着与CAUSAL-UCB相比，CAUSAL-TS在计算上更可行。

## 4 仿真实验

在本节中，我们评估了算法在各种类型的因果图中随机生成的SCM上的性能。毕竟，我们的算法能够一致地找到具有混合范围的对应最优策略。进一步利用底层环境中的因果关系加速了在线学习者的收敛速度。在所有实验中，我们评估了新颖的CAUSAL-TS*，其对外生概率使用无信息Dirichlet先验，对结构函数使用均匀先验，我们将其标记为c-ts*。作为基准，我们还包括在所有可能范围内均匀随机分配治疗的随机试验（rct）、使用所有确定性策略作为臂的标准Thompson抽样算法（ts），以及在简化混合范围上进行Thompson抽样的算法（ts*），该简化混合范围是通过应用中的图形条件获得的。对于所有算法，我们在T = 1.1 × 10^3回合中测量其累积遗憾。有关实验设置的详细讨论，请参阅附录B。

### 实验1

再次考虑图1中描述的因果图和策略范围。我们在图1a中随机生成了100个SCM实例，其中二元变量X1, X2, Z, W, Y ∈ {0, 1}。外生变量U1, U2是离散的，在有限定义域中取值，基数分别为d1 = 16和d2 = 48。定理2和命题1表明，这个参数化家族足以生成图1中候选策略的所有可能影响奖励。图3a报告了随机SCM的平均累积遗憾。我们的分析表明，c-ts*由于能够利用底层因果知识重用由混合范围策略引起的样本， consistently outperform了ts和ts*。由于范围S1, S2, S1,2没有一个是始终冗余或次优的，ts和ts*的表现一致。毫不奇怪，rct在所有算法中表现最差。

### 实验2

考虑图4中的因果图，其中Y代表心血管疾病，W代表血压，X1代表服用抗高血压药物，X2代表使用抗糖尿病药物[30]。我们的目标是评估范围为S1 = {hX1, ∅i}，S2 = {hX2, ∅i}和S1,2 = {hX1, ∅i, hX2, {X1}i}的策略。我们在图4中随机生成了100个SCM实例，其中二元变量X1, X2, W, Y ∈ {0, 1}。外生变量U1, U2, U3从有限定义域的分布中抽取，基数分别为d1 = 8，d2 = 12，d3 = 16。根据定理2，这种参数化家族能够生成所有具有范围S1, S2, S1,2的策略的期望奖励。仿真结果如图3b所示，揭示了ts和ts的性能一致；因果方法c-ts始终优于ts和ts；最后，rct在所有策略中表现最差。

### 实验3

考虑图2中的因果图和策略范围S1 = {hX1, {Z}i}，S2 = {hX2, {Z}i}和S1,2 = {hX1, {Z}i, hX2, {X1, Z}i}。我们在图2中随机生成了100个SCM实例，其中二元变量X1, X2, Z, Y ∈ {0, 1}。外生变量U1, U2从定义域基数分别为d1 = 2和d2 = 24的分类分布中抽取。图3c显示了所有算法在随机SCM上的平均累积遗憾。仿真结果显示，c-ts始终优于其他算法；ts由于不探索冗余的策略范围S1,2，表现优于ts。正如预期的那样，rct在所有学习策略中表现最差。

## 5 结论

本文研究了从一个具有混合状态-动作范围的策略空间中选择最优治疗方案的在线强化学习。我们首先提出了一种在线算法（算法1），该算法实现了次线性遗憾，但对于任何中等规模的实例来说计算上是不可行的。随后，我们引入了一种新的参数化方法（定理2），适用于一般的SCM，这种方法具有有限的观测和潜在域，可以使用最少的分解因子（称为c-组件）来表示因果图中的任意干预分布子集。然后，我们开发了一种基于Thompson抽样启发式的计算效率更高的在线算法（算法3），它能够以相同的样本复杂度识别最优策略。在当今的医疗保健中，组合疗法的日益使用通过结合多种治疗方法为设计有效的治疗方案带来了新的机遇。这些额外的自由度在比较不同治疗方案时带来了挑战。我们相信，我们的研究结果是朝着发展更加有原则的个性化医学科学迈出的重要一步。